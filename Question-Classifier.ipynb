{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import spacy \n",
    "import json \n",
    "from pandas.io.json import json_normalize\n",
    "from tqdm.auto import tqdm \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import swifter\n",
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cb9d07e9d04018bc0e7c7104b9876d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=442.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b60168167f4c28a942d674f81c7132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856a5fa7be8e401eb7c2a0cd9933b643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=239915.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate question data\n",
    "extracted_rows = []\n",
    "\n",
    "# question text file into csv\n",
    "with open(\"train_5500.label.txt\", \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "    i = 0\n",
    "    for l in f.readlines():\n",
    "        split_line = l.split()\n",
    "        extracted_rows.append({\"text\": \" \".join(split_line[1:]), \"label\":1})\n",
    "\n",
    "# statement & question text file into csv\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data = json.load(open(\"train-v2.0.json\", \"r\"))\n",
    "data.keys()\n",
    "\n",
    "# remove punctuation and part of speech \n",
    "def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch.lower() for ch in text if ch not in exclude)\n",
    "\n",
    "def clean_str(text: str) -> str:\n",
    "    return \" \".join([tok.lemma_ for tok in nlp(text)])\n",
    "\n",
    "for article in tqdm(data['data']):\n",
    "    for paragraph in article['paragraphs']:\n",
    "        for sent in nlp(paragraph['context']).sents:\n",
    "            extracted_rows.append({\"text\": sent.text, \"label\": 0})\n",
    "        for i, question in enumerate(paragraph['qas']):\n",
    "            if i % 3 == 0:\n",
    "                extracted_rows.append({\"text\": remove_punc(question['question']), \"label\": 1})\n",
    "            else:\n",
    "                extracted_rows.append({\"text\": question['question'], \"label\": 1})\n",
    "\n",
    "# statement text file into csv            \n",
    "data2 = json.load(open(\"dev-v2.0.json\", \"r\"))\n",
    "\n",
    "for article in tqdm(data2['data']):\n",
    "    for paragraph in article['paragraphs']:\n",
    "        for sent in nlp(paragraph['context']).sents:\n",
    "            extracted_rows.append({\"text\": sent.text, \"label\": 0})\n",
    "\n",
    "# part of speech cleaning\n",
    "for i in tqdm(range(0, len(extracted_rows))):\n",
    "    extracted_rows[i]['text'] = clean_str(extracted_rows[i]['text'])\n",
    "    \n",
    "table = pd.DataFrame(extracted_rows)\n",
    "table.to_csv(\"qa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>104144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>135771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text\n",
       "label        \n",
       "0      104144\n",
       "1      135771"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         text\n",
      "label        \n",
      "0       78108\n",
      "1      101828\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>26036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>33943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text\n",
       "label       \n",
       "0      26036\n",
       "1      33943"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a model \n",
    "train, test = train_test_split(table, stratify = table['label'], random_state = 75)\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(table['text'])\n",
    "\n",
    "X_train = tfidf.transform(train['text'])\n",
    "X_test = tfidf.transform(test['text'])\n",
    "\n",
    "print(train.groupby(['label']).count())\n",
    "test.groupby(['label']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.76      0.83     26036\n",
      "           1       0.84      0.95      0.89     33943\n",
      "\n",
      "    accuracy                           0.87     59979\n",
      "   macro avg       0.88      0.85      0.86     59979\n",
      "weighted avg       0.87      0.87      0.86     59979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# multinomial fitting \n",
    "clf = MultinomialNB(alpha = 0.6)\n",
    "clf.fit(X_train, train['label'])\n",
    "clf.score(X_train, train['label'])\n",
    "\n",
    "mpreds = clf.predict(X_test)\n",
    "print(classification_report(test['label'], mpreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98     26036\n",
      "           1       0.98      0.98      0.98     33943\n",
      "\n",
      "    accuracy                           0.98     59979\n",
      "   macro avg       0.98      0.98      0.98     59979\n",
      "weighted avg       0.98      0.98      0.98     59979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# logistic regression fitting\n",
    "lclf = LogisticRegression()\n",
    "lclf.fit(X_train, train['label'])\n",
    "lclf.score(X_train, train['label'])\n",
    "\n",
    "lpreds = lclf.predict(X_test)\n",
    "print(classification_report(test['label'], lpreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96     26036\n",
      "           1       0.97      0.97      0.97     33943\n",
      "\n",
      "    accuracy                           0.96     59979\n",
      "   macro avg       0.96      0.96      0.96     59979\n",
      "weighted avg       0.96      0.96      0.96     59979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rclf = RandomForestClassifier()\n",
    "rclf.fit(X_train, train['label'])\n",
    "rclf.score(X_train, train['label'])\n",
    "\n",
    "rpreds = rclf.predict(X_test)\n",
    "print(classification_report(test['label'], rpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# apply model to posh data\n",
    "message = pd.read_csv(\"all_user_questions.csv\")\n",
    "message['message'] = message['message'].apply(lambda x: str(x)) \n",
    "\n",
    "m_encoded = tfidf.transform(message['message'].tolist())\n",
    "\n",
    "# get predictions and scores\n",
    "preds = lclf.predict(m_encoded)\n",
    "scores = lclf.predict_proba(m_encoded)\n",
    " \n",
    "message[\"is_question\"] = preds\n",
    "message['is_question_score'] = np.max(scores, axis = 1)\n",
    "\n",
    "# get random sample and export to csv\n",
    "m2 = message.sample(n=300, random_state = 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get classification report\n",
    "m2new = m2.drop_duplicates(subset='message', keep='first')\n",
    "m2new.to_csv(\"sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89       140\n",
      "           1       0.78      0.93      0.84        82\n",
      "\n",
      "    accuracy                           0.87       222\n",
      "   macro avg       0.86      0.88      0.87       222\n",
      "weighted avg       0.89      0.87      0.88       222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get classification report\n",
    "ms = pd.read_csv(\"sample2.csv\")\n",
    "print(classification_report(ms['is_question'], ms['gold_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# apply model to posh data\n",
    "message = pd.read_csv(\"all_user_questions.csv\")\n",
    "message['message'] = message['message'].apply(lambda x: str(x)) \n",
    "\n",
    "m_encoded = tfidf.transform(message['message'].tolist())\n",
    "\n",
    "# get predictions and scores\n",
    "preds = rclf.predict(m_encoded)\n",
    "scores = rclf.predict_proba(m_encoded)\n",
    " \n",
    "message[\"is_question\"] = preds\n",
    "message['is_question_score'] = np.max(scores, axis = 1)\n",
    "\n",
    "# get random sample and export to csv\n",
    "m2 = message.sample(n=300, random_state = 2000)\n",
    "m2.to_csv(\"rsample.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       162\n",
      "           1       0.81      0.83      0.82       138\n",
      "\n",
      "    accuracy                           0.83       300\n",
      "   macro avg       0.83      0.83      0.83       300\n",
      "weighted avg       0.83      0.83      0.83       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ms = pd.read_csv(\"rsample2.csv\")\n",
    "print(classification_report(ms['is_question'], ms['gold_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:   36.1s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   44.9s\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  5.6min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create first pipeline for base without reducing features.\n",
    "pipe = Pipeline([('classifier' , LogisticRegression())])\n",
    "\n",
    "# Create param grid.\n",
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : ['l1', 'l2'],\n",
    "    'classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'classifier__solver' : ['liblinear']}]\n",
    "\n",
    "y_train = train[\"label\"]\n",
    "\n",
    "# Create grid search object\n",
    "gclf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=10, n_jobs=-1)\n",
    "\n",
    "# Fit on data\n",
    "best_clf = gclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     26036\n",
      "           1       0.98      0.98      0.98     33943\n",
      "\n",
      "    accuracy                           0.98     59979\n",
      "   macro avg       0.98      0.98      0.98     59979\n",
      "weighted avg       0.98      0.98      0.98     59979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_clf = gclf.best_estimator_\n",
    "preds = best_clf.predict(X_test)\n",
    "\n",
    "y_test = test['label']\n",
    "preds = best_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# apply model to posh data\n",
    "message = pd.read_csv(\"all_user_questions.csv\")\n",
    "message['message'] = message['message'].apply(lambda x: str(x)) \n",
    "\n",
    "m2_encoded = tfidf.transform(message['message'].tolist())\n",
    "\n",
    "# get predictions and scores\n",
    "preds = best_clf.predict(m2_encoded)\n",
    "scores = best_clf.predict_proba(m2_encoded)\n",
    " \n",
    "message[\"is_question\"] = preds\n",
    "message['is_question_score'] = np.max(scores, axis = 1)\n",
    "\n",
    "# get random sample and export to csv\n",
    "m3 = message.sample(n=300, random_state = 2000)\n",
    "m3new = m3.drop_duplicates(subset='message', keep='first')\n",
    "m3new.to_csv(\"sample3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89       140\n",
      "           1       0.78      0.91      0.84        82\n",
      "\n",
      "    accuracy                           0.87       222\n",
      "   macro avg       0.86      0.88      0.87       222\n",
      "weighted avg       0.88      0.87      0.88       222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ms = pd.read_csv(\"sample3.csv\")\n",
    "print(classification_report(ms['is_question'], ms['gold_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Productionalize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "class QuestionClassifier:\n",
    "    def __init__(self, model_path: str = None,\n",
    "               train_mode = True):\n",
    "        self.model = None\n",
    "        self.text_featurizer = None\n",
    "        self.label_encoder = None\n",
    "\n",
    "        if model_path:\n",
    "          self.load(model_path)\n",
    "\n",
    "        self.train_mode = train_mode\n",
    "\n",
    "    def load(self, model_path: str) -> bool:\n",
    "        print(\"loading :\", model_path)\n",
    "        loaded_model = pickle.load(open(model_path, \"rb\"))\n",
    "        self.model = loaded_model[\"model\"]\n",
    "        self.text_featurizer = loaded_model[\"text_featurizer\"]\n",
    "        self.train_mode = False\n",
    "        return True\n",
    "\n",
    "    def save(self, model_path: str, model_name: str) -> str:\n",
    "        model_properties = {\"model\": self.model,\n",
    "                            \"text_featurizer\": self.text_featurizer}\n",
    "        filename = model_path+model_name+\".pkl\"\n",
    "        pickle.dump(model_properties, open(filename,\"wb\"))\n",
    "        return filename\n",
    "\n",
    "    def remove_punc(self, text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch.lower() for ch in text if ch not in exclude)\n",
    "\n",
    "    def clean_str(self, text: str) -> str:\n",
    "        return \" \".join([tok.lemma_ for tok in nlp(text)])\n",
    "\n",
    "    def train(self, X: List[str], y: List[int] ) -> float:\n",
    "        \"\"\"\" Take in X,y. Fit model to data and return\n",
    "             the model's train accuracy \"\"\"\n",
    "        # Preprocess texts by removing punc and lemmatizing\n",
    "\n",
    "        X = [clean_str(remove_punc(i)) for i in X]\n",
    "\n",
    "        # 1. Clean X and convert features\n",
    "        self.text_featurizer = TfidfVectorizer()\n",
    "        self.text_featurizer.fit(X)\n",
    "\n",
    "        X_train = self.text_featurizer.transform(X)\n",
    "\n",
    "        # 2. Remove Punctuation & Part of Speech cleaning\n",
    "        self.remove_punc = remove_punc(X)\n",
    "        self.clean = clean_str(y)\n",
    "\n",
    "        y_train = self.clean.transform(y)\n",
    "\n",
    "        # 3. Load and fit model to X,y \n",
    "        self.model  = LogisticRegression()\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "        # 4. Calcuate train accuracy\n",
    "        train_acc = self.model.score(X_train, y_train)\n",
    "        return train_acc\n",
    "  \n",
    "    def predict(self, inputs: List[str]) -> List[str]:\n",
    "        \"\"\" Take in a list of string inputs and output\n",
    "            a list of the model's predictions \"\"\"\n",
    "\n",
    "        # 1. Convert inputs into features\n",
    "        input_feats = self.text_featurizer.transform(inputs)\n",
    "\n",
    "        # 2. Run model on features and get predictions\n",
    "        preds = self.model.predict(input_feats)\n",
    "\n",
    "        # 3. Convert predictions into clean labels to return\n",
    "        pred_labels = self.label_encoder.inverse_transform(preds)\n",
    "\n",
    "        return pred_labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
